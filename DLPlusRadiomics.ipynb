{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fusion_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfusion_model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenseNet121\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fusion_model'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,classification_report,recall_score,precision_score\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import fusion_model\n",
    "\n",
    "import json\n",
    "from monai.networks.nets import DenseNet121\n",
    "import torch\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import RandRotate90, Resize, EnsureChannelFirst, Compose, ScaleIntensity,RandAxisFlip\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(acc_list,auc_list,recall_list,precision_list):    \n",
    "    acc_list = np.array(acc_list)\n",
    "    auc_list = np.array(auc_list)\n",
    "    recall_list = np.array(recall_list)\n",
    "    precision_list = np.array(precision_list)\n",
    "    print(f'Recall, Average:{recall_list.mean():.4f}, Std:{recall_list.std():.4f}')\n",
    "    print(f'precision, Average:{precision_list.mean():.4f}, Std:{precision_list.std():.4f}')\n",
    "    print(f'Accuracy, Average:{acc_list.mean():.4f}, Std:{acc_list.std():.4f}')\n",
    "    print(f'AUC, Average:{auc_list.mean():.4f}, Std:{auc_list.std():.4f}')\n",
    "\n",
    "def get_data_list(dataset_dtl, images_path,labels_path,fold):\n",
    "\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    df = pd.read_csv(labels_path)\n",
    "    df_cleaned = df.dropna(subset=[df.columns[1]]) # remove NaN\n",
    "\n",
    "    train_list,val_list, test_list = get_tr_vl_ts_list(dataset_dtl,fold)\n",
    "    df_train = df_cleaned[df_cleaned['name'].isin(train_list)]\n",
    "    df_val = df_cleaned[df_cleaned['name'].isin(val_list)]\n",
    "    df_test = df_cleaned[df_cleaned['name'].isin(test_list)]\n",
    "\n",
    "    df_train['path'] = df_train['name'].apply(lambda x: os.path.join(images_path, x+'.nii.gz'))\n",
    "    df_test['path'] = df_test['name'].apply(lambda x: os.path.join(images_path, x+'.nii.gz'))\n",
    "    df_val['path'] = df_val['name'].apply(lambda x: os.path.join(images_path, x+'.nii.gz'))\n",
    "    \n",
    "def get_tr_vl_ts_list(dataset_dtl,fold=0):\n",
    "\n",
    "    with open(dataset_dtl, 'r') as f:\n",
    "        fold_data = json.load(f)\n",
    "    test_list = []\n",
    "    for name in fold_data['test_files']:\n",
    "        test_list.append(name.split('.nii.gz')[0])\n",
    "    # test_list = [n.lower() for n in test_list]\n",
    "\n",
    "    train_list =[]\n",
    "    for name in fold_data['cross_validation'][fold]['train_files']:\n",
    "        train_list.append(name.split('.nii.gz')[0])\n",
    "    # train_list = [n.lower() for n in train_list]\n",
    "\n",
    "\n",
    "    val_list=[]\n",
    "    for name in fold_data['cross_validation'][fold]['validation_files']:\n",
    "        val_list.append(name.split('.nii.gz')[0])\n",
    "    # val_list = [n.lower() for n in val_list]\n",
    "    \n",
    "    return train_list,val_list,test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_densenet_probabilities(densenet_model, test_dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad(): \n",
    "        progress_bar = tqdm(test_dataloader, desc=\"Testing\")\n",
    "    for X, y in progress_bar:\n",
    "        X = X.to('cuda')\n",
    "        pred = densenet_model(X)\n",
    "        prediction_list.append(torch.nn.functional.softmax(pred, dim=-1).cpu().detach().numpy().reshape(2))\n",
    "    return np.array(prediction_list)\n",
    "\n",
    "def get_rf_probabilities(model, radiomics_features_normed):\n",
    "    probs = model.predict_proba(radiomics_features_normed)  # Output probabilities for each class\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fusion_model(dl_model, rf_model, image_dataloader, radiomics_data,labels):\n",
    "    dl_probs = get_densenet_probabilities(dl_model, image_dataloader)\n",
    "    rf_probs = get_rf_probabilities(rf_model, radiomics_data)\n",
    "    fusion_features = np.hstack([dl_probs, rf_probs])\n",
    "    # fusion_features = np.hstack([densenet_probs[:,1].reshape(len(labels),1), rf_probs[:,1].reshape(len(labels),1)])\n",
    "    print(fusion_features.shape)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(fusion_features, labels)\n",
    "    return model\n",
    "\n",
    "def predict_with_fusion_model(densenet_model, rf_model, fusion_model,image_dataloader, radiomics_data):\n",
    "    densenet_probs = get_densenet_probabilities(densenet_model, image_dataloader)\n",
    "    rf_probs = get_rf_probabilities(rf_model, radiomics_data)\n",
    "    # Combine probabilities for the fusion model  \n",
    "    fusion_features = np.hstack([densenet_probs, rf_probs])\n",
    "    # fusion_features = np.hstack([densenet_probs[:,1].reshape(len(radiomics_data),1), rf_probs[:,1].reshape(len(radiomics_data),1)])\n",
    "    fusion_predictoin = fusion_model.predict(fusion_features)\n",
    "    fusion_proba = fusion_model.predict_proba(fusion_features)\n",
    "    return fusion_predictoin,fusion_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_info = 'Train_Test_1'\n",
    "\n",
    "dataset_dtl_path = f'/home/pyq6817/IPMN-Radiomics-Plus-Deeplearning/{train_test_info}.json'\n",
    "# deep learning input\n",
    "input_path = '/data/Ziliang/IPMN_cysts_20240909/deeplearning/ROI'\n",
    "label_path = '/home/pyq6817/IPMN-Radiomics-Plus-Deeplearning/labels.csv'\n",
    "\n",
    "dl_model_dir = '/data/Ziliang/IPMN_cysts_20240909/DenseNet121_weights'\n",
    "radiomcis_model_dir = '/home/pyq6817/IPMN-Radiomics-Plus-Deeplearning/radiomics/trained_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read radiomics data and correspond features.\n",
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4\n",
    "# feature_list = [\n",
    "# 'skewness-Laws R5S5',\n",
    "# 'Collage_kurt_MaximalCorrelationCoefficient_1_nb_4_ws_5',\n",
    "# 'Collage_skew_InformationMeasureOfCorrelation2_1_nb_8_ws_3',\n",
    "# 'Collage_kurt_Contrast_1_nb_8_ws_7',\n",
    "# 'Collage_median_MaximalCorrelationCoefficient_1_nb_32_ws_3',\n",
    "# 'Collage_skew_Correlation_1_nb_16_ws_5',\n",
    "# 'Collage_skew_MaximalCorrelationCoefficient_1_nb_4_ws_3',\n",
    "# 'Collage_skew_SumVariance_1_nb_8_ws_5',\n",
    "# 'median-Laws E5L5',\n",
    "# 'Collage_skew_Entropy_1_nb_16_ws_7',\n",
    "# 'skewness-Laws S5E5',\n",
    "# 'skewness-Laws W5L5',\n",
    "# 'Collage_skew_DifferenceEntropy_1_nb_4_ws_3',\n",
    "# 'Collage_var_Contrast_1_nb_8_ws_7',\n",
    "# 'Collage_kurt_SumEntropy_1_nb_16_ws_5',\n",
    "# ]\n",
    "\n",
    "# test3\n",
    "# feature_list = ['Collage_skew_InformationMeasureOfCorrelation2_1_nb_8_ws_3',\n",
    "# 'skewness-Laws S5E5',\n",
    "# 'skewness-Laws R5S5',\n",
    "# 'Collage_skew_InformationMeasureOfCorrelation2_1_nb_16_ws_5',\n",
    "# 'Collage_skew_Correlation_1_nb_8_ws_5',\n",
    "# 'median-Laws E5L5',\n",
    "# 'median-Laws W5S5',\n",
    "# 'Collage_skew_MaximalCorrelationCoefficient_1_nb_4_ws_3']\n",
    "\n",
    "# test2\n",
    "# feature_list = ['Collage_kurt_MaximalCorrelationCoefficient_1_nb_4_ws_5',\n",
    "# 'median-Laws E5L5',\n",
    "# 'Collage_skew_InformationMeasureOfCorrelation2_1_nb_8_ws_3',\n",
    "# 'Collage_var_InformationMeasureOfCorrelation2_1_nb_16_ws_3',\n",
    "# 'skewness-Laws R5S5',\n",
    "# 'Collage_skew_MaximalCorrelationCoefficient_1_nb_4_ws_3',\n",
    "# 'skewness-Laws S5E5',\n",
    "# 'median-Laws W5S5',\n",
    "# 'Collage_kurt_MaximalCorrelationCoefficient_1_nb_4_ws_7',\n",
    "# 'Collage_median_SumEntropy_1_nb_8_ws_3',\n",
    "# 'Collage_kurt_SumEntropy_1_nb_16_ws_5',\n",
    "# 'Collage_skew_SumVariance_1_nb_4_ws_7']\n",
    "\n",
    "# test1\n",
    "feature_list = ['Collage_skew_InformationMeasureOfCorrelation2_1_nb_8_ws_3',\n",
    "'Collage_var_InformationMeasureOfCorrelation2_1_nb_16_ws_3',\n",
    "'Collage_kurt_Contrast_1_nb_16_ws_7',\n",
    "'Collage_skew_MaximalCorrelationCoefficient_1_nb_4_ws_3',\n",
    "'Collage_kurt_SumAverage_1_nb_16_ws_3',\n",
    "'Collage_kurt_InformationMeasureOfCorrelation2_1_nb_16_ws_3',\n",
    "'Collage_kurt_MaximalCorrelationCoefficient_1_nb_4_ws_5',\n",
    "'median-Laws E5L5',\n",
    "'Collage_kurt_SumEntropy_1_nb_16_ws_3',\n",
    "'skewness-Laws S5E5',\n",
    "'kurtosis-Haralick correlation ws=7 n=4',\n",
    "'Collage_kurt_InformationMeasureOfCorrelation2_1_nb_64_ws_5',\n",
    "'Collage_median_DifferenceVariance_1_nb_64_ws_7',\n",
    "'skewness-Laws E5S5',\n",
    "'skewness-Gradient sobelxy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/tmp/ipykernel_88441/3109599350.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['path'] =test_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n"
     ]
    }
   ],
   "source": [
    "# Get features\n",
    "data = pd.read_csv('./Data/2D_t2/all.csv')\n",
    "data[\"Label\"] = data['Label'].apply(lambda x: x-1)\n",
    "data[\"Name\"] = data['Name'].str.replace(r'^IU_', 'IUC_', regex=True)\n",
    "\n",
    "train_list,val_list,test_list = get_tr_vl_ts_list(dataset_dtl=dataset_dtl_path,fold=0)\n",
    "# train_data = data[data['Name'].isin(train_list)]\n",
    "test_data = data[data['Name'].isin(test_list)] # This is for validation\n",
    "# val_data = data[data['Name'].isin(val_list)]\n",
    "train_val_data = data[data['Name'].isin(val_list+train_list)]\n",
    "\n",
    "# Use train and val data to build scalar\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "train_val_data.drop(columns=['Center','Name','Label'])\n",
    "x_train_val_raw = train_val_data.drop(columns=['Center','Name','Label'])\n",
    "y_train_val = train_val_data['Label']\n",
    "x_train_val_scaled = pd.DataFrame(standard_scaler.fit_transform(x_train_val_raw),columns=x_train_val_raw.columns)\n",
    "x_train_val = x_train_val_scaled[feature_list]\n",
    "\n",
    "# select \n",
    "x_test_raw = test_data.drop(columns=['Center','Name','Label'])\n",
    "y_test = test_data[[\"Label\"]]\n",
    "x_test_scaled = pd.DataFrame(standard_scaler.transform(x_test_raw),columns=x_test_raw.columns)\n",
    "x_test = x_test_scaled[feature_list]\n",
    "test_data['path'] =test_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = Compose([ScaleIntensity(), EnsureChannelFirst(), Resize((96, 96, 96))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88441/3592759472.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['path'] = train_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n",
      "Testing: 100%|██████████| 262/262 [00:21<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 29/29 [00:02<00:00, 10.42it/s]\n",
      "/tmp/ipykernel_88441/3592759472.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['path'] = train_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 - ACC: 0.4483, AUC: 0.4216\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 264/264 [00:20<00:00, 12.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 29/29 [00:02<00:00, 11.33it/s]\n",
      "/tmp/ipykernel_88441/3592759472.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['path'] = train_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - ACC: 0.4483, AUC: 0.4608\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 263/263 [00:22<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 29/29 [00:03<00:00,  9.25it/s]\n",
      "/tmp/ipykernel_88441/3592759472.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['path'] = train_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 - ACC: 0.4483, AUC: 0.4853\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 263/263 [00:21<00:00, 12.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 29/29 [00:02<00:00, 10.62it/s]\n",
      "/tmp/ipykernel_88441/3592759472.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['path'] = train_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 - ACC: 0.5517, AUC: 0.4902\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 264/264 [00:20<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 29/29 [00:02<00:00, 11.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4 - ACC: 0.4828, AUC: 0.4559\n",
      "----------------------------------------\n",
      "Validation set\n",
      "Recall, Average:nan, Std:nan\n",
      "precision, Average:nan, Std:nan\n",
      "Accuracy, Average:nan, Std:nan\n",
      "AUC, Average:nan, Std:nan\n",
      "Test set\n",
      "Recall, Average:nan, Std:nan\n",
      "precision, Average:nan, Std:nan\n",
      "Accuracy, Average:0.4759, Std:0.0402\n",
      "AUC, Average:0.4627, Std:0.0245\n",
      "================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_88441/161524014.py:6: RuntimeWarning: Mean of empty slice.\n",
      "  print(f'Recall, Average:{recall_list.mean():.4f}, Std:{recall_list.std():.4f}')\n",
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/data2/pyq6817/.conda/envs/medical-image/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_88441/161524014.py:7: RuntimeWarning: Mean of empty slice.\n",
      "  print(f'precision, Average:{precision_list.mean():.4f}, Std:{precision_list.std():.4f}')\n",
      "/tmp/ipykernel_88441/161524014.py:8: RuntimeWarning: Mean of empty slice.\n",
      "  print(f'Accuracy, Average:{acc_list.mean():.4f}, Std:{acc_list.std():.4f}')\n",
      "/tmp/ipykernel_88441/161524014.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  print(f'AUC, Average:{auc_list.mean():.4f}, Std:{auc_list.std():.4f}')\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "test_acc_list = []\n",
    "test_auc_list = []\n",
    "test_recall_list = []\n",
    "test_precision_list = []\n",
    "val_acc_list = []\n",
    "val_auc_list = []\n",
    "val_recall_list = []\n",
    "val_precision_list = []\n",
    "\n",
    "\n",
    "for fold in range(5):\n",
    "    radiomics_model = load(os.path.join(radiomcis_model_dir,f'{train_test_info}/{train_test_info}_Fold{fold+1}.joblib'))\n",
    "    train_list,val_list,_ = get_tr_vl_ts_list(dataset_dtl=dataset_dtl_path,fold=fold)\n",
    "\n",
    "    train_data = data[data['Name'].isin(train_list)]\n",
    "    val_data = data[data['Name'].isin(val_list)]\n",
    "\n",
    "    x_train_raw = train_data.drop(columns=['Center','Name','Label'])\n",
    "    # y_train = train_data[[\"Label\"]]\n",
    "    x_train_scaled = pd.DataFrame(standard_scaler.transform(x_train_raw),columns=x_train_raw.columns)\n",
    "    x_train = x_train_scaled[feature_list]\n",
    "    train_data['path'] = train_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n",
    "\n",
    "    \n",
    "\n",
    "    # x_val_raw = val_data.drop(columns=['Center','Name','Label'])\n",
    "    # y_val = val_data[[\"Label\"]]\n",
    "    # x_val_scaled = pd.DataFrame(standard_scaler.transform(x_val_raw),columns=x_val_raw.columns)\n",
    "    # x_val = x_val_scaled[feature_list]\n",
    "    # val_data['path'] = val_data['Name'].apply(lambda x: os.path.join(input_path, x+'.nii.gz'))\n",
    "    param_grid = {\n",
    "    'k': np.linspace(0, 1, 10),  # 10 values from 0 to 1\n",
    "    't': np.linspace(0, 1, 10)  # 10 values from 0 to 1\n",
    "    }\n",
    "\n",
    "    densenet = DenseNet121(\n",
    "            spatial_dims=3,  # 3D input\n",
    "            in_channels=1,   # Typically for grayscale (e.g., MRI/CT scans), change to 3 for RGB\n",
    "            out_channels=2   # Adjust for binary or multi-class segmentation/classification\n",
    "        )\n",
    "    \n",
    "    \n",
    "    densenet.load_state_dict(torch.load(os.path.join(dl_model_dir,f'model_auc_{train_test_info}_fold{fold}.pth'), map_location='cpu', weights_only=True))\n",
    "    densenet.to('cuda')\n",
    "    \n",
    "    train_ds = ImageDataset(image_files=train_data['path'].to_list(), labels=train_data['Label'].to_list(), transform=test_transforms)\n",
    "    test_ds = ImageDataset(image_files=test_data['path'].to_list(), labels=test_data['Label'].to_list(), transform=test_transforms)\n",
    "    \n",
    "    test_dataloader = DataLoader(test_ds, batch_size=1, shuffle=False,num_workers=1)\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=1, shuffle=False,num_workers=1)\n",
    "    \n",
    "    fusion_model = train_fusion_model(dl_model=densenet, rf_model=radiomics_model, \n",
    "                       image_dataloader=train_dataloader, radiomics_data=x_train,\n",
    "                       labels=train_data['Label'])\n",
    "    \n",
    "    y_pred,y_prob = predict_with_fusion_model(densenet_model = densenet, rf_model = radiomics_model, fusion_model = fusion_model,\n",
    "                                 image_dataloader = test_dataloader, radiomics_data = x_test)\n",
    "\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_acc_list.append(test_accuracy)\n",
    "    test_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "    test_auc_list.append(test_auc)\n",
    "    print(f\"Test {fold} - ACC: {test_accuracy:.4f}, AUC: {test_auc:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "print('Validation set')\n",
    "get_results(val_acc_list,val_auc_list,val_recall_list,val_precision_list)\n",
    "print(\"Test set\")\n",
    "get_results(test_acc_list,test_auc_list,test_recall_list,test_precision_list)\n",
    "print('================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "Recall, Average:nan, Std:nan\n",
      "precision, Average:nan, Std:nan\n",
      "Accuracy, Average:0.4759, Std:0.0402\n",
      "AUC, Average:0.4627, Std:0.0245\n",
      "================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88441/161524014.py:6: RuntimeWarning: Mean of empty slice.\n",
      "  print(f'Recall, Average:{recall_list.mean():.4f}, Std:{recall_list.std():.4f}')\n",
      "/tmp/ipykernel_88441/161524014.py:7: RuntimeWarning: Mean of empty slice.\n",
      "  print(f'precision, Average:{precision_list.mean():.4f}, Std:{precision_list.std():.4f}')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Test set\")\n",
    "get_results(test_acc_list,test_auc_list,test_recall_list,test_precision_list)\n",
    "print('================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
